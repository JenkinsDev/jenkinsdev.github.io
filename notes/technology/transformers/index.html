<!doctype html><html lang=en><head><title>Transformers :: JenkinsDev :: Into the mind of an aspiring polymathic autodidact</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="What is it? A deep learning architecture which is based on multi-head attention.
Transformers were introduced to the world through a 2017 paper by eight scientists at Google: “Attention Is All You Need”. A paper which is seen as the turning point of modern artificial intelligence.
Why was it created? Previously, ML architectures such as recurrent architectures, long short-term memory took much longer to train.
Transformers enabled more efficient training, and by proxy made possible the wave of LLMs (Large Language Models) we have access to today."><meta name=keywords content="technology coding programming software development creativity work family life everything"><meta name=robots content="noodp"><link rel=canonical href=https://jenkinsdev.us/notes/technology/transformers/><link rel=stylesheet href=https://jenkinsdev.us/assets/style.css><link rel=stylesheet href=https://jenkinsdev.us/assets/blue.css><link rel=apple-touch-icon href=https://jenkinsdev.us/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://jenkinsdev.us/img/favicon/blue.png><meta name=twitter:card content="summary"><meta name=twitter:site content="jenkinsdev.github.io"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Transformers"><meta property="og:description" content="What is it? A deep learning architecture which is based on multi-head attention.
Transformers were introduced to the world through a 2017 paper by eight scientists at Google: “Attention Is All You Need”. A paper which is seen as the turning point of modern artificial intelligence.
Why was it created? Previously, ML architectures such as recurrent architectures, long short-term memory took much longer to train.
Transformers enabled more efficient training, and by proxy made possible the wave of LLMs (Large Language Models) we have access to today."><meta property="og:url" content="https://jenkinsdev.us/notes/technology/transformers/"><meta property="og:site_name" content="JenkinsDev :: Into the mind of an aspiring polymathic autodidact"><meta property="og:image" content="https://jenkinsdev.us/img/favicon/blue.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2024-02-27 20:30:15 +0000 UTC"><script async src="https://www.googletagmanager.com/gtag/js?id=G-8HW9H1ENZ4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8HW9H1ENZ4")</script></head><body class=blue><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>JenkinsDev</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/conway.html>Conway's Game of Life (☑️ Edition)</a></li><li><a href=/notes>Public Notes</a></li><li><a href=/showcase>Showcase</a></li><li><a href=/why-hire-me>Why Hire Me?</a></li><ul class=menu__sub-inner><li class=menu__sub-inner-more-trigger>Show more ▾</li><ul class="menu__sub-inner-more hidden"><li><a href=/about>About</a></li><li><a href=/contact-me>Contact Me</a></li></ul></ul></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/conway.html>Conway's Game of Life (☑️ Edition)</a></li><li><a href=/notes>Public Notes</a></li><li><a href=/showcase>Showcase</a></li><li><a href=/why-hire-me>Why Hire Me?</a></li><li><a href=/about>About</a></li><li><a href=/contact-me>Contact Me</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://jenkinsdev.us/notes/technology/transformers/>Transformers</a></h1><div class=post-meta><span class=post-date>2024-02-27
</span><span class=post-reading-time>:: 1 min read (87 words)</span></div><span class=post-tags>#<a href=https://jenkinsdev.us/tags/engineering/>engineering</a>&nbsp;
#<a href=https://jenkinsdev.us/tags/machine-learning/>machine-learning</a>&nbsp;
#<a href=https://jenkinsdev.us/tags/artifical-intelligence/>artifical-intelligence</a>&nbsp;
#<a href=https://jenkinsdev.us/tags/deep-learning/>deep-learning</a>&nbsp;</span><div class=post-content><div><h3 id=what-is-it>What is it?<a href=#what-is-it class=hanchor arialabel=Anchor>&#8983;</a></h3><p>A deep learning architecture which is based on multi-head attention.</p><p>Transformers were introduced to the world through a 2017 paper by eight scientists at Google: <a href=https://en.m.wikipedia.org/wiki/Attention_Is_All_You_Need>“Attention Is All You Need”</a>. A paper which is seen as the turning point of modern artificial intelligence.</p><h3 id=why-was-it-created>Why was it created?<a href=#why-was-it-created class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Previously, ML architectures such as recurrent architectures, long short-term memory took much longer to train.</p><p>Transformers enabled more efficient training, and by proxy made possible the wave of LLMs (Large Language Models) we have access to today.</p></div></div></div></div><footer class=footer></footer><script src=https://jenkinsdev.us/assets/main.js></script><script src=https://jenkinsdev.us/assets/prism.js></script></div></body></html>