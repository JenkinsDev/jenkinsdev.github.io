<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep-learning on JenkinsDev :: Into the mind of an aspiring polymathic autodidact</title><link>https://jenkinsdev.us/tags/deep-learning/</link><description>Recent content in deep-learning on JenkinsDev :: Into the mind of an aspiring polymathic autodidact</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 22 Nov 2024 14:46:51 +0000</lastBuildDate><atom:link href="https://jenkinsdev.us/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Foundation Models</title><link>https://jenkinsdev.us/notes/technology/foundation-models/</link><pubDate>Fri, 22 Nov 2024 14:46:51 +0000</pubDate><guid>https://jenkinsdev.us/notes/technology/foundation-models/</guid><description>A type of ML model that is trained on a broad dataset. These models are generally used for general or broad use cases, and may be adaptable through fine-tuning.
Foundation models like the ones powering OpenAI’s ChatGPT, can cost hundreds of millions of dollars to train.
The term was coined in Aug. 2021 at Stanford.
——
Foundation models are near universally based on Transformers.</description><content>&lt;p>A type of ML model that is trained on a broad dataset. These models are generally used for general or broad use cases, and may be adaptable through fine-tuning.&lt;/p>
&lt;p>Foundation models like the ones powering OpenAI’s ChatGPT, can cost hundreds of millions of dollars to train.&lt;/p>
&lt;p>The term was coined in Aug. 2021 at Stanford.&lt;/p>
&lt;p>——&lt;/p>
&lt;p>Foundation models are near universally based on &lt;a href="https://jenkinsdev.us/notes/technology/transformers/">Transformers&lt;/a>.&lt;/p></content></item><item><title>Mamba Model</title><link>https://jenkinsdev.us/notes/technology/mamba-model/</link><pubDate>Fri, 22 Nov 2024 14:46:51 +0000</pubDate><guid>https://jenkinsdev.us/notes/technology/mamba-model/</guid><description>Why? Foundation Models
Linear Attention Gated Convolution Recurrent Models</description><content>&lt;h3 id="why">Why?&lt;/h3>
&lt;p>&lt;a href="https://jenkinsdev.us/notes/technology/foundation-models/">Foundation Models&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Linear Attention&lt;/li>
&lt;li>Gated Convolution&lt;/li>
&lt;li>Recurrent Models&lt;/li>
&lt;/ul></content></item><item><title>Transformers</title><link>https://jenkinsdev.us/notes/technology/transformers/</link><pubDate>Fri, 22 Nov 2024 14:46:51 +0000</pubDate><guid>https://jenkinsdev.us/notes/technology/transformers/</guid><description>What is it? A deep learning architecture which is based on multi-head attention.
Transformers were introduced to the world through a 2017 paper by eight scientists at Google: “Attention Is All You Need”. A paper which is seen as the turning point of modern artificial intelligence.
Why was it created? Previously, ML architectures such as recurrent architectures, long short-term memory took much longer to train.
Transformers enabled more efficient training, and by proxy made possible the wave of LLMs (Large Language Models) we have access to today.</description><content>&lt;h3 id="what-is-it">What is it?&lt;/h3>
&lt;p>A deep learning architecture which is based on multi-head attention.&lt;/p>
&lt;p>Transformers were introduced to the world through a 2017 paper by eight scientists at Google: &lt;a href="https://en.m.wikipedia.org/wiki/Attention_Is_All_You_Need">“Attention Is All You Need”&lt;/a>. A paper which is seen as the turning point of modern artificial intelligence.&lt;/p>
&lt;h3 id="why-was-it-created">Why was it created?&lt;/h3>
&lt;p>Previously, ML architectures such as recurrent architectures, long short-term memory took much longer to train.&lt;/p>
&lt;p>Transformers enabled more efficient training, and by proxy made possible the wave of LLMs (Large Language Models) we have access to today.&lt;/p></content></item></channel></rss>